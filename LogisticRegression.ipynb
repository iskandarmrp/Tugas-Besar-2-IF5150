{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RandomForest import RandomForest\n",
    "from NaiveBayes import NaiveBayes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, ephocs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.ephocs = ephocs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.randomForest = RandomForest(3)\n",
    "        self.naiveBayes = NaiveBayes()\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + math.exp(-1*z))\n",
    "\n",
    "    def initialize_weights(self, n_features):\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        self.initialize_weights(2)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        X_train_nb = X_train\n",
    "\n",
    "        X_test_nb = X_test\n",
    "\n",
    "        label_encoder = LabelEncoder()\n",
    "\n",
    "        for col in range(X.shape[1]):\n",
    "            label_encoder.fit(X[:, col])\n",
    "            \n",
    "            X_train_nb[:, col] = label_encoder.transform(X_train_nb[:, col])\n",
    "            X_test_nb[:, col] = label_encoder.transform(X_test_nb[:, col])\n",
    "\n",
    "        self.randomForest.fit(X_train, y_train)\n",
    "\n",
    "        self.naiveBayes.fit(X_train_nb, y_train)\n",
    "\n",
    "        X = np.vstack((X_train, X_test))\n",
    "        X_nb = np.vstack([X_train_nb, X_test_nb]) \n",
    "        y = np.hstack((y_train, y_test))\n",
    "\n",
    "        print(type(X_nb))\n",
    "        df = pd.DataFrame({'Feature_1': [None] * n_samples})\n",
    "        for i in range(n_samples):\n",
    "            row = pd.DataFrame([X[i]])\n",
    "            df.at[i, 'Feature_1'] = self.randomForest.predict(row)\n",
    "        df['Feature_2'] = self.naiveBayes.predict(X_nb)\n",
    "\n",
    "        for _ in range(self.ephocs):\n",
    "            print(df)\n",
    "            linear_model = np.dot(df, self.weights) + self.bias\n",
    "            print(linear_model)\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        x_1 = self.randomForest.predict(X)     \n",
    "\n",
    "        x_2 = self.naiveBayes.predict(X)\n",
    "\n",
    "        x = np.array([x_1,x_2])\n",
    "\n",
    "        linear_model = np.dot(x, self.weights) + self.bias\n",
    "            \n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "        return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clean_data.csv')\n",
    "\n",
    "selected_features = [\n",
    "    'is_canceled',\n",
    "    'hotel',\n",
    "    'stays_in_weekend_nights',\n",
    "    'adults',\n",
    "    'children',\n",
    "    'market_segment',\n",
    "    'total_of_special_requests',\n",
    "    'is_repeated_guest',\n",
    "    'previous_cancellations',\n",
    "    'previous_bookings_not_canceled',\n",
    "    'deposit_type',\n",
    "    'total_orang'\n",
    "]\n",
    "\n",
    "selected_df = df[selected_features]\n",
    "\n",
    "X, y = selected_df.drop(columns=['is_canceled']).values, selected_df['is_canceled'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=11, step=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.vstack((X_train, X_test))\n",
    "row = pd.DataFrame([X[0]])\n",
    "row.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "loop of ufunc does not support argument 0 of type float which has no callable exp method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'exp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 48\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     46\u001b[0m     row \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([X[i]])\n\u001b[0;32m     47\u001b[0m     df\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature_1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandomForest\u001b[38;5;241m.\u001b[39mpredict(row)\n\u001b[1;32m---> 48\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature_2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnaiveBayes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_nb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mephocs):\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df)\n",
      "File \u001b[1;32mf:\\pindahan\\S2\\AI Lanjut\\Tubes\\Tubes2\\Tugas-Besar-2-IF5150\\NaiveBayes.py:26\u001b[0m, in \u001b[0;36mNaiveBayes.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m---> 26\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X]\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(y_pred)\n",
      "File \u001b[1;32mf:\\pindahan\\S2\\AI Lanjut\\Tubes\\Tubes2\\Tugas-Besar-2-IF5150\\NaiveBayes.py:35\u001b[0m, in \u001b[0;36mNaiveBayes._predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_classes):\n\u001b[0;32m     34\u001b[0m     prior \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_priors[idx])\n\u001b[1;32m---> 35\u001b[0m     posterior \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     36\u001b[0m     posterior \u001b[38;5;241m=\u001b[39m posterior \u001b[38;5;241m+\u001b[39m prior\n\u001b[0;32m     37\u001b[0m     posteriors\u001b[38;5;241m.\u001b[39mappend(posterior)\n",
      "File \u001b[1;32mf:\\pindahan\\S2\\AI Lanjut\\Tubes\\Tubes2\\Tugas-Besar-2-IF5150\\NaiveBayes.py:47\u001b[0m, in \u001b[0;36mNaiveBayes._pdf\u001b[1;34m(self, class_idx, x)\u001b[0m\n\u001b[0;32m     45\u001b[0m var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var[class_idx]\n\u001b[0;32m     46\u001b[0m var \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(var, \u001b[38;5;241m1e-9\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m numerator \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m denominator \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m*\u001b[39m var)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmaximum(numerator \u001b[38;5;241m/\u001b[39m denominator, \u001b[38;5;241m1e-9\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type float which has no callable exp method"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
